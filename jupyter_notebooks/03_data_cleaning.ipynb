{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "* Evaluate missing data.\n",
    "* Clean data.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* inputs/datasets/raw/house-price-20211124T154130Z-001/house-price/house_prices_records.csv\n",
    "* inputs/datasets/raw/house-price-20211124T154130Z-001/house-price/inherited_houses.csv\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* Test set: outputs/datasets/cleaned/test_set.csv\n",
    "* Train set: outputs/datasets/cleaned/train_set.csv\n",
    "* outputs/datasets/cleaned/clean_house_price_records.csv\n",
    "* outputs/datasets/cleaned/clean_inherited_houses.csv\n",
    "\n",
    "### Conclusions \n",
    "\n",
    "* Data Cleaning Pipeline.\n",
    "\n",
    "### Additional Comments\n",
    "\n",
    "* This file and its contents were inspired by and adapted from the Churnometer Walkthrough Project 2 and other lessons from Code Institute.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "from ydata_profiling import ProfileReport\n",
    "from feature_engine.imputation import ArbitraryNumberImputer, CategoricalImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"inputs/datasets/raw/house-price-20211124T154130Z-001/house-price/house_prices_records.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inherited = pd.read_csv(f\"inputs/datasets/raw/house-price-20211124T154130Z-001/house-price/inherited_houses.csv\")\n",
    "print(df_inherited.shape)\n",
    "df_inherited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Identifying Columns with Missing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "vars_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vars_missing_data:\n",
    "   pandas_report = ProfileReport(df=df[vars_missing_data], minimal=True)\n",
    "   pandas_report.to_notebook_iframe()\n",
    "else:\n",
    "   print(\"There are no variables with missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and PPS Analysis\n",
    "\n",
    "In this section, we aim to analyze the correlation between the target variable, SalePrice, and other features. We\"ll use Pearson and Spearman correlation heatmaps, as well as a PPS heatmap, based on the PPS lesson to visualize these relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(20,12), font_annot = 8):\n",
    "  if len(df.columns) > 1:\n",
    "    mask = np.zeros_like(df, dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    mask[abs(df) < threshold] = True\n",
    "\n",
    "    fig, axes = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                mask=mask, cmap=\"viridis\", annot_kws={\"size\": font_annot}, ax=axes,\n",
    "                linewidth=0.5\n",
    "                     )\n",
    "    axes.set_yticklabels(df.columns, rotation = 0)\n",
    "    plt.ylim(len(df.columns),0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20,12), font_annot = 8):\n",
    "    if len(df.columns) > 1:\n",
    "\n",
    "      mask = np.zeros_like(df, dtype=bool)\n",
    "      mask[abs(df) < threshold] = True\n",
    "\n",
    "      fig, ax = plt.subplots(figsize=figsize)\n",
    "      ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                       mask=mask,cmap=\"rocket_r\", annot_kws={\"size\": font_annot},\n",
    "                       linewidth=0.05, linecolor=\"grey\")\n",
    "      \n",
    "      plt.ylim(len(df.columns),0)\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    # Filter out only numeric columns for correlation calculations\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Calculate Spearman and Pearson correlations\n",
    "    df_corr_spearman = numeric_df.corr(method=\"spearman\")\n",
    "    df_corr_pearson = numeric_df.corr(method=\"pearson\")\n",
    "\n",
    "    # Calculate the PPS matrix\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter([\"x\", \"y\", \"ppscore\"]).pivot(columns=\"x\", index=\"y\", values=\"ppscore\")\n",
    "\n",
    "    # Calculate PPS score statistics for thresholding\n",
    "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter([\"ppscore\"]).describe().T\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(20,12), font_annot=8 ):\n",
    "\n",
    "  print(\"\\n\")\n",
    "  print(\"* Here I can analyze how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "  print(\"* Analyze multi colinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "  print(\"\\n\")\n",
    "  print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "  print(\"It evaluates monotonic relationships between variables \\n\")\n",
    "  heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "  print(\"\\n\")\n",
    "  print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "  print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "  heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "  print(\"\\n\")\n",
    "  print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "  print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "        f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "  heatmap_pps(df=pps_matrix,threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of PPS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display at Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCorrAndPPS(df_corr_pearson=df_corr_pearson,\n",
    "                  df_corr_spearman=df_corr_spearman,\n",
    "                  pps_matrix=pps_matrix,\n",
    "                  CorrThreshold=0.6,\n",
    "                  PPS_Threshold=0.2,\n",
    "                  figsize=(12, 10),\n",
    "                  font_annot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Missing Data Levels\n",
    "\n",
    "* Custom function to display missing data levels in a DataFrame, it shows the absolute levels, relative levels and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMissingData(df):\n",
    "    missing_data_absolute = df.isnull().sum()\n",
    "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
    "    df_missing_data = (pd.DataFrame(\n",
    "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
    "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
    "                                   \"DataType\": df.dtypes}\n",
    "                                    )\n",
    "                          .sort_values(by=[\"PercentageOfDataset\"], ascending=False)\n",
    "                          .query(\"PercentageOfDataset > 0\")\n",
    "                          )\n",
    "\n",
    "    return df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing data levels for the collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create copy of house price dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set, _, __ = train_test_split(\n",
    "                                        df,\n",
    "                                        df[\"SalePrice\"],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0)\n",
    "\n",
    "print(f\"train_set shape: {train_set.shape} \\ntest_set shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = EvaluateMissingData(train_set)\n",
    "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
    "df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Variables\n",
    "\n",
    "First we identify Variables with more than 80% missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "missing_fraction = train_set.isna().mean()\n",
    "variables_to_drop = missing_fraction[missing_fraction > threshold].index.tolist()\n",
    "\n",
    "print(f\"Variables with more than 80% missing data: {variables_to_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deeper analysis of missing Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enclosedporch = train_set.loc[train_set[\"EnclosedPorch\"].notnull()]\n",
    "df_enclosedporch[[\"EnclosedPorch\", \"SalePrice\"]].plot(kind=\"scatter\", x=\"EnclosedPorch\", y=\"SalePrice\")\n",
    "\n",
    "df_wooddecksf = train_set.loc[train_set[\"WoodDeckSF\"].notnull()]\n",
    "df_wooddecksf[[\"WoodDeckSF\", \"SalePrice\"]].plot(kind=\"scatter\", x=\"WoodDeckSF\", y=\"SalePrice\")\n",
    "\n",
    "df_wooddecksf[\"WoodDeckSF\"].value_counts().sort_index(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Variables with High Missing Values\n",
    "After analyzing the dataset, we found that the following variables had more than 80% missing data:\n",
    "- `EnclosedPorch`\n",
    "- `WoodDeckSF`\n",
    "\n",
    "These variables are unlikely to add significant value to our model due to the high proportion of missing data. Therefore, we decided to drop them from both the training and test datasets.\n",
    "\n",
    "We then re-evaluated the dataset to check if any variables still have missing data and will proceed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "variables_to_drop = [\"EnclosedPorch\", \"WoodDeckSF\"]\n",
    "\n",
    "imputer = DropFeatures(features_to_drop=variables_to_drop)\n",
    "imputer.fit(train_set)\n",
    "train_set, test_set = imputer.transform(train_set), imputer.transform(test_set)\n",
    "train_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop features from inherited dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = imputer.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_variables = train_set.columns[train_set.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop LotFrontage and MasVnrArea from the analysis due to their low correlation with SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"LotFrontage\"].value_counts().sort_index(ascending=False).head()\n",
    "train_set[\"MasVnrArea\"].value_counts().sort_index(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute Median value into null variables using MeanMedianImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import MeanMedianImputer\n",
    "variables = [\"LotFrontage\", \"MasVnrArea\"]\n",
    "imputer = MeanMedianImputer(imputation_method=\"median\", variables=variables)\n",
    "imputer.fit(train_set)\n",
    "train_set, test_set = imputer.transform(train_set), imputer.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = imputer.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Remove EnclosedPorch and WoodDeckSF and imputed missing values for LotFrontage and MasVnrArea. These features no longer appear in our analysis list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second floor size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"2ndFlrSF\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 626 houses have 0 square feet on the second floor, indicating that these houses are single-story.\n",
    "\n",
    "* There are 345 unique values in total, meaning houses with a second floor vary widely in size.\n",
    "\n",
    "* The remaining values (besides 0) each appear only once, indicating a high variability in the second-floor area for houses that have it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bedrooms above grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"BedroomAbvGr\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3 bedrooms above ground is the most common, with 599 occurrences.\n",
    "\n",
    "* 2 bedrooms is the second most common, appearing 266 times.\n",
    "\n",
    "* 4 bedrooms is also relatively frequent, with 166 occurrences.\n",
    "\n",
    "* Smaller counts are seen for 1-bedroom (32), 5-bedroom (16), 0-bedroom (4), and 6-bedroom (4) houses.\n",
    "\n",
    "* There is one house with 8 bedrooms above ground, making it a rare case in the dataset.\n",
    "\n",
    "This distribution highlights that most houses have 2â€“4 bedrooms above ground, with very few houses outside this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "      ( \"2ndFlrSF\",  ArbitraryNumberImputer(arbitrary_number=0,\n",
    "                                                variables=[\"2ndFlrSF\", \"BedroomAbvGr\"]) )\n",
    "])\n",
    "pipeline\n",
    "\n",
    "pipeline.fit(train_set)\n",
    "train_set, test_set = pipeline.transform(train_set), pipeline.transform(test_set)\n",
    "df_clean = pipeline.transform(df_clean)\n",
    "EvaluateMissingData(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If `BsmtExposure` is \"None\", assume there is no basement, and fill with \"None\".\n",
    "- For rows where `BsmtFinSF1` is 0 (indicating unfinished area), fill with \"Unf\".\n",
    "- For remaining missing values, fill with \"Unk\" to indicate unknown status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"BsmtFinType1\"] = np.where(\n",
    "    (train_set[\"BsmtExposure\"] == \"None\") & (train_set[\"BsmtFinType1\"].isnull()),\n",
    "    \"None\",\n",
    "    train_set[\"BsmtFinType1\"]\n",
    ")\n",
    "\n",
    "train_set[\"BsmtFinType1\"] = np.where(\n",
    "    (train_set[\"BsmtFinSF1\"] == 0) & (train_set[\"BsmtFinType1\"].isnull()),\n",
    "    \"Unf\",\n",
    "    train_set[\"BsmtFinType1\"]\n",
    ")\n",
    "\n",
    "pipeline_bsmtfintype1 = Pipeline([\n",
    "    (\"categorical_imputer\", CategoricalImputer(imputation_method=\"missing\", fill_value=\"Unk\", variables=[\"BsmtFinType1\"]))\n",
    "])\n",
    "\n",
    "pipeline_bsmtfintype1.fit(train_set)\n",
    "train_set = pipeline_bsmtfintype1.transform(train_set)\n",
    "EvaluateMissingData(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If `BsmtFinType1` is \"None\" (indicating no basement), fill with \"None\" to indicate the absence of basement exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_bsmtexposure = Pipeline([\n",
    "    (\"categorical_imputer\", CategoricalImputer(imputation_method=\"missing\", fill_value=\"None\", variables=[\"BsmtExposure\"]))\n",
    "])\n",
    "\n",
    "pipeline_bsmtexposure.fit(train_set)\n",
    "train_set = pipeline_bsmtexposure.transform(train_set)\n",
    "EvaluateMissingData(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If `GarageArea` is 0 (indicating no garage), fill with \"None\".\n",
    "- For remaining missing values, assume the garage is unfinished, and fill with \"Unf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"GarageFinish\"] = np.where(\n",
    "    (train_set[\"GarageArea\"] == 0) & (train_set[\"GarageFinish\"].isnull()),\n",
    "    \"None\",\n",
    "    train_set[\"GarageFinish\"]\n",
    ")\n",
    "\n",
    "pipeline_garagefinish = Pipeline([\n",
    "    (\"categorical_imputer\", CategoricalImputer(imputation_method=\"missing\", fill_value=\"Unf\", variables=[\"GarageFinish\"]))\n",
    "])\n",
    "\n",
    "pipeline_garagefinish.fit(train_set)\n",
    "train_set = pipeline_garagefinish.transform(train_set)\n",
    "EvaluateMissingData(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `GarageYrBlt` column represents the year the garage was built. If `GarageFinish` is \"None\" (indicating no garage), fill with 0 to mark the absence of a garage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_garageyrblt = Pipeline([\n",
    "    (\"arbitrary_number_imputer\", ArbitraryNumberImputer(arbitrary_number=0, variables=[\"GarageYrBlt\"]))\n",
    "])\n",
    "\n",
    "pipeline_garageyrblt.fit(train_set)\n",
    "train_set = pipeline_garageyrblt.transform(train_set)\n",
    "EvaluateMissingData(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have any missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to se if datasets contain float columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_float_columns(df, df_name):\n",
    "    float_cols = df.select_dtypes(include=\"float\").columns\n",
    "    if len(float_cols) > 0:\n",
    "        print(f\"The following float columns are present in {df_name}:\")\n",
    "        for col in float_cols:\n",
    "            print(f\" - {col}\")\n",
    "    else:\n",
    "        print(f\"No float columns found in {df_name}.\")\n",
    "\n",
    "list_float_columns(df_clean, \"df_clean\")\n",
    "list_float_columns(df_inherited, \"df_inherited\")\n",
    "list_float_columns(train_set, \"train_set\")\n",
    "list_float_columns(test_set, \"test_set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert float to int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_floats_to_int(df):\n",
    "    float_cols = df.select_dtypes(\"float\").columns\n",
    "    for col in float_cols:\n",
    "        if (df[col] % 1 == 0).all():  \n",
    "            df[col] = df[col].astype(\"int64\")  \n",
    "    return df\n",
    "\n",
    "print(\"Original shape of df_clean:\", df_clean.shape)\n",
    "df_clean = convert_floats_to_int(df_clean)\n",
    "print(\"df_clean after conversion:\")\n",
    "print(df_clean.select_dtypes(\"float\").info())  \n",
    "\n",
    "print(\"Original shape of df_inherited:\", df_inherited.shape)\n",
    "df_inherited = convert_floats_to_int(df_inherited)\n",
    "print(\"df_inherited after conversion:\")\n",
    "print(df_inherited.select_dtypes(\"float\").info())  \n",
    "\n",
    "print(\"Original shape of train_set:\", train_set.shape)\n",
    "train_set = convert_floats_to_int(train_set)\n",
    "print(\"train_set after conversion:\")\n",
    "print(train_set.select_dtypes(\"float\").info())\n",
    "\n",
    "print(\"Original shape of test_set:\", test_set.shape)\n",
    "test_set = convert_floats_to_int(test_set)\n",
    "print(\"test_set after conversion:\")\n",
    "print(test_set.select_dtypes(\"float\").info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GarageYrBlt still contain a float. We convert it to an int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"GarageYrBlt\"] = df_clean[\"GarageYrBlt\"].fillna(0)\n",
    "df_inherited[\"GarageYrBlt\"] = df_inherited[\"GarageYrBlt\"].fillna(0)\n",
    "train_set[\"GarageYrBlt\"] = train_set[\"GarageYrBlt\"].fillna(0)\n",
    "test_set[\"GarageYrBlt\"] = test_set[\"GarageYrBlt\"].fillna(0)\n",
    "\n",
    "df_clean[\"GarageYrBlt\"] = df_clean[\"GarageYrBlt\"].astype(int)\n",
    "df_inherited[\"GarageYrBlt\"] = df_inherited[\"GarageYrBlt\"].astype(int)\n",
    "train_set[\"GarageYrBlt\"] = train_set[\"GarageYrBlt\"].astype(int)\n",
    "test_set[\"GarageYrBlt\"] = test_set[\"GarageYrBlt\"].astype(int)\n",
    "\n",
    "print(\"df_clean info after GarageYrBlt conversion:\")\n",
    "print(df_clean.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"df_inherited info after GarageYrBlt conversion:\")\n",
    "print(df_inherited.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"train_set info after GarageYrBlt conversion:\")\n",
    "print(train_set.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"test_set info after GarageYrBlt conversion:\")\n",
    "print(test_set.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All missing values have been successfully handled, and no columns contain null values. Additionally, all relevant float columns have been converted to int, ensuring a consistent data format across the dataset for further analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Train and Test sets to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  os.makedirs(name=\"outputs/datasets/cleaned\") \n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(\"outputs/datasets/cleaned/train_set.csv\", index=False)\n",
    "test_set.to_csv(\"outputs/datasets/cleaned/test_set.csv\", index=False)\n",
    "df_clean.to_csv(\"outputs/datasets/cleaned/clean_house_price_records.csv\", index=False)\n",
    "df_inherited.to_csv(\"outputs/datasets/cleaned/clean_inherited_houses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and next step\n",
    "\n",
    "The data cleaning process successfully handled all missing values and converted relevant float columns to integers, ensuring a consistent dataset ready for analysis. Next, we will proceed with Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
