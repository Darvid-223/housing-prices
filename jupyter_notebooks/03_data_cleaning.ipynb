{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "* Evaluate missing data.\n",
    "* Clean data.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* inputs/datasets/raw/house-price-20211124T154130Z-001/house-price/house_prices_records.csv\n",
    "* inputs/datasets/raw/house-price-20211124T154130Z-001/house-price/inherited_houses.csv\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* Train set: outputs/datasets/cleaned/TrainSetCleaned.csv \n",
    "* Test set: outputs/datasets/cleaned/TestSetCleaned.csv\n",
    "\n",
    "### Conclusions \n",
    "\n",
    "* Data Cleaning Pipeline.\n",
    "\n",
    "### Additional Comments\n",
    "\n",
    "* This file and its contents were inspired by and adapted from the Churnometer Walkthrough Project 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = (pd.read_csv(\"outputs/datasets/collection/house-prices.csv\")\n",
    "    )\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Identifying Columns with Missing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "if vars_with_missing_data:\n",
    "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
    "    profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"There are no variables with missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "%matplotlib inline\n",
    "\n",
    "# Function to generate a heatmap based on correlation\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)  # np.bool is deprecated, use bool\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=ax,\n",
    "                    linewidth=0.5)\n",
    "        ax.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "# Function to generate a heatmap based on PPS\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)  # np.bool is deprecated, use bool\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
    "                    linewidth=0.05, linecolor='grey')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "# Function to calculate both correlation and PPS (Predictive Power Score)\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    # Calculate Spearman and Pearson correlations\n",
    "    df_corr_spearman = df.corr(method=\"spearman\")\n",
    "    df_corr_pearson = df.corr(method=\"pearson\")\n",
    "\n",
    "    # Calculate PPS matrix\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    # Calculate PPS score statistics to decide threshold\n",
    "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "# Function to display all three heatmaps: Spearman, Pearson, PPS\n",
    "\n",
    "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(20, 12), font_annot=8):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first identify all categorical columns in the dataset using df.select_dtypes() and print them out. Then, we apply pd.get_dummies() to convert specific categorical columns ('BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual') into numerical one-hot encoded columns, while dropping the first category to avoid the dummy variable trap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(categorical_cols)\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Correlations and Power Predictive Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display at Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the correlation and PPS heatmaps\n",
    "DisplayCorrAndPPS(df_corr_pearson=df_corr_pearson,\n",
    "                  df_corr_spearman=df_corr_spearman,\n",
    "                  pps_matrix=pps_matrix,\n",
    "                  CorrThreshold=0.4,  # The threshold to filter the correlations displayed in the heatmap\n",
    "                  PPS_Threshold=0.2,   # The threshold for the PPS score displayed in the heatmap\n",
    "                  figsize=(12, 10),    # Set the figure size for the heatmaps\n",
    "                  font_annot=10)       # Set the font size for the annotations in the heatmaps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Missing Data Levels\n",
    "\n",
    "* Custom function to display missing data levels in a DataFrame, it shows the absolute levels, relative levels and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMissingData(df):\n",
    "    missing_data_absolute = df.isnull().sum()\n",
    "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
    "    df_missing_data = (pd.DataFrame(\n",
    "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
    "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
    "                                   \"DataType\": df.dtypes}\n",
    "                                    )\n",
    "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
    "                          .query(\"PercentageOfDataset > 0\")\n",
    "                          )\n",
    "\n",
    "    return df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing data levels for the collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowsWithMissingData</th>\n",
       "      <th>PercentageOfDataset</th>\n",
       "      <th>DataType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <td>1324</td>\n",
       "      <td>90.68</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <td>1305</td>\n",
       "      <td>89.38</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>259</td>\n",
       "      <td>17.74</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageFinish</th>\n",
       "      <td>235</td>\n",
       "      <td>16.10</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <td>145</td>\n",
       "      <td>9.93</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <td>99</td>\n",
       "      <td>6.78</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <td>86</td>\n",
       "      <td>5.89</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <td>81</td>\n",
       "      <td>5.55</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtExposure</th>\n",
       "      <td>38</td>\n",
       "      <td>2.60</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrArea</th>\n",
       "      <td>8</td>\n",
       "      <td>0.55</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RowsWithMissingData  PercentageOfDataset DataType\n",
       "EnclosedPorch                 1324                90.68  float64\n",
       "WoodDeckSF                    1305                89.38  float64\n",
       "LotFrontage                    259                17.74  float64\n",
       "GarageFinish                   235                16.10   object\n",
       "BsmtFinType1                   145                 9.93   object\n",
       "BedroomAbvGr                    99                 6.78  float64\n",
       "2ndFlrSF                        86                 5.89  float64\n",
       "GarageYrBlt                     81                 5.55  float64\n",
       "BsmtExposure                    38                 2.60   object\n",
       "MasVnrArea                       8                 0.55  float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvaluateMissingData(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Spreadsheet Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column          | Type of Cleaning                | Description of Action                                                   | Justification                          |\n",
    "|-----------------|---------------------------------|-------------------------------------------------------------------------|----------------------------------------|\n",
    "| `Working Directory` | Change Working Directory     | Set current directory to project root                                  | Ensure all file paths are relative and consistent |\n",
    "| `house-prices.csv`  | Load Data                    | Loaded data from CSV using Pandas                                      | Begin data exploration and processing  |\n",
    "| `Missing Values`    | Identify Missing Values      | Identified columns with missing values using `.isna()`                 | Understand data quality and plan for handling missing data |\n",
    "| `BsmtExposure`, `BsmtFinType1`, `GarageFinish`, `KitchenQual` | One-Hot Encoding | Converted categorical columns to numerical representation using `pd.get_dummies()` | Machine Learning models require numerical data  |\n",
    "| `Pearson and Spearman Correlation` | Calculate Correlations | Used `.corr()` to calculate Pearson and Spearman correlations          | Understand relationships between features for feature selection |\n",
    "| `PPS Matrix`     | Calculate PPS                  | Used `ppscore.matrix()` to evaluate predictive power between features  | Detect both linear and non-linear relationships |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "TrainSet, TestSet, _, __ = train_test_split(\n",
    "                                        df,\n",
    "                                        df['SalePrice'],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0)\n",
    "\n",
    "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = EvaluateMissingData(TrainSet)\n",
    "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
    "df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we identify Variables with more than 80% missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "missing_fraction = TrainSet.isna().mean()\n",
    "variables_to_drop = missing_fraction[missing_fraction > threshold].index.tolist()\n",
    "\n",
    "print(f\"Variables with more than 80% missing data: {variables_to_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Variables with High Missing Values\n",
    "After analyzing the dataset, we found that the following variables had more than 80% missing data:\n",
    "- `EnclosedPorch`\n",
    "- `WoodDeckSF`\n",
    "\n",
    "These variables are unlikely to add significant value to our model due to the high proportion of missing data. Therefore, we decided to drop them from both the training and test datasets.\n",
    "\n",
    "We then re-evaluated the dataset to check if any variables still have missing data and will proceed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_drop = ['EnclosedPorch', 'WoodDeckSF']\n",
    "\n",
    "TrainSet = TrainSet.drop(columns=variables_to_drop)\n",
    "TestSet = TestSet.drop(columns=variables_to_drop)\n",
    "\n",
    "print(f\"* {len(variables_to_drop)} variables have been dropped: {variables_to_drop}\")\n",
    "\n",
    "df_missing_data = TrainSet.isna().sum()\n",
    "print(f\"* There are still {df_missing_data[df_missing_data > 0].shape[0]} variables with missing data \\n\")\n",
    "\n",
    "if df_missing_data[df_missing_data > 0].shape[0] > 0:\n",
    "    print(\"Remaining variables with missing data:\\n\", df_missing_data[df_missing_data > 0])\n",
    "else:\n",
    "    print(\"No variables with missing data remaining.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "After dropping columns with over 80% missing values, we still identified several columns with missing data:\n",
    "- Numerical Columns: `2ndFlrSF`, `BedroomAbvGr`, `GarageYrBlt`, `LotFrontage`, `MasVnrArea`\n",
    "- Categorical Columns: `BsmtExposure`, `BsmtFinType1`, `GarageFinish`\n",
    "\n",
    "To handle the missing data, we decided to:\n",
    "- Impute numerical columns using the median value, as it is less affected by outliers compared to the mean.\n",
    "- Impute categorical columns using the most frequent value (mode), ensuring that the data remains consistent.\n",
    "\n",
    "After performing imputations, we rechecked for missing data to ensure no missing values remain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_cols = ['2ndFlrSF', 'BedroomAbvGr', 'GarageYrBlt', 'LotFrontage', 'MasVnrArea']\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "TrainSet[numerical_cols] = num_imputer.fit_transform(TrainSet[numerical_cols])\n",
    "TestSet[numerical_cols] = num_imputer.transform(TestSet[numerical_cols])\n",
    "\n",
    "\n",
    "categorical_cols = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish']\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "TrainSet[categorical_cols] = cat_imputer.fit_transform(TrainSet[categorical_cols])\n",
    "TestSet[categorical_cols] = cat_imputer.transform(TestSet[categorical_cols])\n",
    "\n",
    "\n",
    "missing_data_after_imputation = TrainSet.isna().sum()\n",
    "print(f\"* After imputation, there are still {missing_data_after_imputation[missing_data_after_imputation > 0].shape[0]} variables with missing data \\n\")\n",
    "\n",
    "if missing_data_after_imputation[missing_data_after_imputation > 0].shape[0] > 0:\n",
    "    print(\"Remaining variables with missing data:\\n\", missing_data_after_imputation[missing_data_after_imputation > 0])\n",
    "else:\n",
    "    print(\"No variables with missing data remaining.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push cleaned data to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  os.makedirs(name='outputs/datasets/cleaned') # create outputs/datasets/collection folder\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
